{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/so-vits-svc-colab/blob/main/so_vits_svc_colab.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ðŸŽ™\n",
        "\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!git clone https://github.com/camenduru/so-vits-svc -b eff-4.0\n",
        "%cd /content/so-vits-svc\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "!pip install -q torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 torchtext==0.14.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu116 -U\n",
        "!pip install -q praat-parselmouth fairseq==0.12.2 huggingface_hub\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/so-vits-svc-test/resolve/main/test_song.wav -d /content -o test_song.wav\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/so-vits-svc-4.0-init/resolve/main/checkpoint_best_legacy_500.pt -d /content/so-vits-svc/hubert -o checkpoint_best_legacy_500.pt\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Itoifi/so-vits-svc-acg-models/resolve/main/44k-V4.0-luna/G_289600.pth -d /content/so-vits-svc/models/luna -o G_289600.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Itoifi/so-vits-svc-acg-models/raw/main/44k-V4.0-luna/config.json -d /content/so-vits-svc/models/luna -o config.json\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import copy\n",
        "import logging\n",
        "import io\n",
        "from ipywidgets import widgets\n",
        "from pathlib import Path\n",
        "from IPython.display import Audio, display\n",
        "import torch\n",
        "from inference import infer_tool\n",
        "from inference import slicer\n",
        "from inference.infer_tool import Svc\n",
        "import soundfile\n",
        "import numpy as np\n",
        "\n",
        "MODELS_DIR = \"models\"\n",
        "\n",
        "def get_speakers():\n",
        "  speakers = []\n",
        "  for _,dirs,_ in os.walk(MODELS_DIR):\n",
        "    for folder in dirs:\n",
        "      cur_speaker = {}\n",
        "      # Look for G_****.pth\n",
        "      g = glob.glob(os.path.join(MODELS_DIR,folder,'G_*.pth'))\n",
        "      if not len(g):\n",
        "        print(\"Skipping \"+folder+\", no G_*.pth\")\n",
        "        continue\n",
        "      cur_speaker[\"model_path\"] = g[0]\n",
        "      cur_speaker[\"model_folder\"] = folder\n",
        "\n",
        "      # Look for *.pt (clustering model)\n",
        "      clst = glob.glob(os.path.join(MODELS_DIR,folder,'*.pt'))\n",
        "      if not len(clst):\n",
        "        print(\"Note: No clustering model found for \"+folder)\n",
        "        cur_speaker[\"cluster_path\"] = \"\"\n",
        "      else:\n",
        "        cur_speaker[\"cluster_path\"] = clst[0]\n",
        "\n",
        "      # Look for config.json\n",
        "      cfg = glob.glob(os.path.join(MODELS_DIR,folder,'*.json'))\n",
        "      if not len(cfg):\n",
        "        print(\"Skipping \"+folder+\", no config json\")\n",
        "        continue\n",
        "      cur_speaker[\"cfg_path\"] = cfg[0]\n",
        "      with open(cur_speaker[\"cfg_path\"]) as f:\n",
        "        try:\n",
        "          cfg_json = json.loads(f.read())\n",
        "        except Exception as e:\n",
        "          print(\"Malformed config json in \"+folder)\n",
        "        for name, i in cfg_json[\"spk\"].items():\n",
        "          cur_speaker[\"name\"] = name\n",
        "          cur_speaker[\"id\"] = i\n",
        "          if not name.startswith('.'):\n",
        "            speakers.append(copy.copy(cur_speaker))\n",
        "\n",
        "    return sorted(speakers, key=lambda x:x[\"name\"].lower())\n",
        "\n",
        "logging.getLogger('numba').setLevel(logging.WARNING)\n",
        "chunks_dict = infer_tool.read_temp(\"inference/chunks_temp.json\")\n",
        "existing_files = []\n",
        "slice_db = -40\n",
        "wav_format = 'wav'\n",
        "\n",
        "class InferenceGui():\n",
        "  def __init__(self):\n",
        "    self.speakers = get_speakers()\n",
        "    self.speaker_list = [x[\"name\"] for x in self.speakers]\n",
        "    self.speaker_box = widgets.Dropdown(\n",
        "        options = self.speaker_list\n",
        "    )\n",
        "    display(self.speaker_box)\n",
        "\n",
        "    def convert_cb(btn):\n",
        "      self.convert()\n",
        "    def clean_cb(btn):\n",
        "      self.clean()\n",
        "\n",
        "    self.convert_btn = widgets.Button(description=\"Convert\")\n",
        "    self.convert_btn.on_click(convert_cb)\n",
        "    self.clean_btn = widgets.Button(description=\"Delete all audio files\")\n",
        "    self.clean_btn.on_click(clean_cb)\n",
        "\n",
        "    self.trans_tx = widgets.IntText(value=0, description='Transpose')\n",
        "    self.cluster_ratio_tx = widgets.FloatText(value=0.0, \n",
        "      description='Clustering Ratio')\n",
        "    self.noise_scale_tx = widgets.FloatText(value=0.4, \n",
        "      description='Noise Scale')\n",
        "    self.auto_pitch_ck = widgets.Checkbox(value=False, description=\n",
        "      'Auto pitch f0 (do not use for singing)')\n",
        "\n",
        "    display(self.trans_tx)\n",
        "    display(self.cluster_ratio_tx)\n",
        "    display(self.noise_scale_tx)\n",
        "    display(self.auto_pitch_ck)\n",
        "    display(self.convert_btn)\n",
        "    display(self.clean_btn)\n",
        "\n",
        "  def convert(self):\n",
        "    trans = int(self.trans_tx.value)\n",
        "    speaker = next(x for x in self.speakers if x[\"name\"] == \n",
        "          self.speaker_box.value)\n",
        "    spkpth2 = os.path.join(os.getcwd(),speaker[\"model_path\"])\n",
        "    print(spkpth2)\n",
        "    print(os.path.exists(spkpth2))\n",
        "\n",
        "    svc_model = Svc(speaker[\"model_path\"], speaker[\"cfg_path\"], \n",
        "      cluster_model_path=speaker[\"cluster_path\"])\n",
        "    \n",
        "    input_filepaths = [f for f in glob.glob('/content/**/*.*', recursive=True)\n",
        "     if f not in existing_files and \n",
        "     any(f.endswith(ex) for ex in ['.wav','.flac','.mp3','.ogg','.opus'])]\n",
        "    for name in input_filepaths:\n",
        "      print(\"Converting \"+os.path.split(name)[-1])\n",
        "      infer_tool.format_wav(name)\n",
        "\n",
        "      wav_path = str(Path(name).with_suffix('.wav'))\n",
        "      wav_name = Path(name).stem\n",
        "      chunks = slicer.cut(wav_path, db_thresh=slice_db)\n",
        "      audio_data, audio_sr = slicer.chunks2audio(wav_path, chunks)\n",
        "\n",
        "      audio = []\n",
        "      for (slice_tag, data) in audio_data:\n",
        "          print(f'#=====segment start, '\n",
        "              f'{round(len(data)/audio_sr, 3)}s======')\n",
        "          \n",
        "          length = int(np.ceil(len(data) / audio_sr *\n",
        "              svc_model.target_sample))\n",
        "          \n",
        "          if slice_tag:\n",
        "              print('jump empty segment')\n",
        "              _audio = np.zeros(length)\n",
        "          else:\n",
        "              # Padding \"fix\" for noise\n",
        "              pad_len = int(audio_sr * 0.5)\n",
        "              data = np.concatenate([np.zeros([pad_len]),\n",
        "                  data, np.zeros([pad_len])])\n",
        "              raw_path = io.BytesIO()\n",
        "              soundfile.write(raw_path, data, audio_sr, format=\"wav\")\n",
        "              raw_path.seek(0)\n",
        "              _cluster_ratio = 0.0\n",
        "              if speaker[\"cluster_path\"] != \"\":\n",
        "                _cluster_ratio = float(self.cluster_ratio_tx.value)\n",
        "              out_audio, out_sr = svc_model.infer(\n",
        "                  speaker[\"name\"], trans, raw_path,\n",
        "                  cluster_infer_ratio = _cluster_ratio,\n",
        "                  auto_predict_f0 = bool(self.auto_pitch_ck.value),\n",
        "                  noice_scale = float(self.noise_scale_tx.value))\n",
        "              _audio = out_audio.cpu().numpy()\n",
        "              pad_len = int(svc_model.target_sample * 0.5)\n",
        "              _audio = _audio[pad_len:-pad_len]\n",
        "          audio.extend(list(infer_tool.pad_array(_audio, length)))\n",
        "          \n",
        "      res_path = os.path.join('/content/',\n",
        "          f'{wav_name}_{trans}_key_'\n",
        "          f'{speaker[\"name\"]}.{wav_format}')\n",
        "      soundfile.write(res_path, audio, svc_model.target_sample,\n",
        "          format=wav_format)\n",
        "      display(Audio(res_path, autoplay=True)) # display audio file\n",
        "    pass\n",
        "\n",
        "  def clean(self):\n",
        "     input_filepaths = [f for f in glob.glob('/content/**/*.*', recursive=True)\n",
        "     if f not in existing_files and \n",
        "     any(f.endswith(ex) for ex in ['.wav','.flac','.mp3','.ogg','.opus'])]\n",
        "     for f in input_filepaths:\n",
        "       os.remove(f)\n",
        "\n",
        "inference_gui = InferenceGui()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gHbYvbmd1_O_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}